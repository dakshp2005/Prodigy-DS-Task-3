# Task 3

<i>Build a decision tree classifier to predict whether a customer will purchase a product or service based on their demographic and behavioral data. Use a dataset such as the Bank Marketing dataset from the UCI Machine LearningÂ Repository.</i>
## About the dataset:

**The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).**


| Variable Name | Role    | Type        | Demographic      | Description                                                                                                                                       | Units |
|---------------|---------|-------------|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-------|
| age           | Feature | Integer     | Age              |                                                                                                                                                   |       |
| job           | Feature | Categorical | Occupation       | type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown') |       |
| marital       | Feature | Categorical | Marital Status   | marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)                                  |       |
| education     | Feature | Categorical | Education Level  | (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')                      |       |
| default       | Feature | Binary      |                  | has credit in default?                                                                                                                             |       |
| balance       | Feature | Integer     |                  | average yearly balance                                                                                                                             | euros |
| housing       | Feature | Binary      |                  | has housing loan?                                                                                                                                  |       |
| loan          | Feature | Binary      |                  | has personal loan?                                                                                                                                 |       |
| contact       | Feature | Categorical |                  | contact communication type (categorical: 'cellular','telephone')                                                                                   |       |
| day_of_week   | Feature | Date        |                  | last contact day of the week                                                                                                                       |       |
| month         | Feature | Date        |                  | last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')                                                                   |       |
| duration      | Feature | Integer     |                  | last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. |       |
| campaign      | Feature | Integer     |                  | number of contacts performed during this campaign and for this client (numeric, includes last contact)                                              |       |
| pdays         | Feature | Integer     |                  | number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)  |       |
| previous      | Feature | Integer     |                  | number of contacts performed before this campaign and for this client                                                                              |       |
| poutcome      | Feature | Categorical |                  | outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')                                                        |       |
| y             | Target  | Binary      |                  | has the client subscribed a term deposit?                                                                                                          |       |

##  Importing the necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
## Reading the dataset
df = pd.read_csv(r"C:\Users\TUFAN\Downloads\Prodigy_InfoTech\Task_3\bank-additional-full.csv",delimiter=';')
df.head(5)
df.rename(columns={'y':'subscribed_deposit'}, inplace=True)
## Details about the dataset
df.shape
df.columns
df.info()
df.describe()
## Checking for null/missing values in the dataset
df.isnull().sum()
`Observation & Inference:`
- No missing values present
## Checking for duplicate values in the dataset
df.duplicated().sum()
df.drop_duplicates(inplace=True)
df.duplicated().sum()
`Observation & Inference:`
- All the duplicate values are removed
## Visualizing numerical columns using histogram
df_obj= df.select_dtypes(include='object').columns

df_num= df.select_dtypes(exclude='object').columns

for feature in df_num:
    sns.histplot(x=feature,data=df,bins=25,kde=True,color='#5f366e')
    plt.show()
## Visualizing categorical columns using bar graphs

for feature in df_obj:
    plt.figure(figsize=(8,3))
    plt.title(f"Count plot of {feature}")
    sns.countplot(x=feature,data=df,palette='viridis')
    plt.xticks(rotation=40)
    plt.show()
`Observations & Insights:`

- In the Job Column, we have seen most of the clients are working as 'admin'.

- In the marital Column, we have seen most of the clients are married.

- In the education Column, we have seen most of the clients are having 'university.degree' as education.

- In the default Column, we have seen most of the clients are having 'no' credit as default.

- In the housing Column, we have seen most of the clients are taking housing loan.

- In the loan Column, we have seen most of the clients are not taking personal loan.

- In the contact Column, we have seen most of the clients are choosen cellular as contact.

- In the month Column, we have seen most of the clients are contacted in the 'may' month.

- In the day_of_week Column, we have seen most of the clients are contacted in 'thursday'.

- In the poutcome Column, we have seen the result of most of the previous market campaign is 'nonexistent'.

- In the target column , we have seen most of the clients are not subscribed a term deposit.
## checking for outliers
df.plot(kind='box', subplots=True, layout=(5,2), figsize=(10,30))
plt.show()
## Outlier treatment

columns = ['age', 'campaign', 'duration']

for column in columns:
    q1 = np.percentile(df[column], 25)
    q3 = np.percentile(df[column], 75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    # Filter the DataFrame for the current column
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]


df.plot(kind='box', subplots=True, layout=(5,2), figsize=(10,30))
plt.show()
## Checking for correlation using heatmap
# Select only the numerical columns
numerical_df = df.select_dtypes(include=[np.number])

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix Heatmap')
plt.show()
high_corr_cols = ['emp.var.rate','euribor3m','nr.employed']
# copy the original dataframe

df1=df.copy()

# Removing high correlated columns from the dataset
df1.drop(high_corr_cols, inplace=True, axis=1)
df1.columns
## showing dimensions of the updated dataset
df1.shape
## Conversion of categorical columns into numerical columns using label encoder.
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_encoded = df1.apply(le.fit_transform)
df_encoded
## Checking the target variable
df_encoded['subscribed_deposit'].value_counts(normalize=True)*100
## independent variables
x = df_encoded.iloc[:,:-1]   

## Target variable
y = df_encoded.iloc[:,-1]     

x.shape
y.shape
## Splitting the dataset into train and test datasets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
## Decision Tree classifier

### using 'gini' criterion
from sklearn.tree import DecisionTreeClassifier
dc = DecisionTreeClassifier(criterion='gini',max_depth=5,min_samples_split=10)
dc.fit(x_train,y_train)
## Evaluating Training and Testing Accuracy
print("Training accuracy:",dc.score(x_train,y_train))
print("Testing accuracy:",dc.score(x_test,y_test))
y_pred=dc.predict(x_test)
## Evaluating Prediction Accuracy
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
# Plot Decision Tree
from sklearn.tree import plot_tree
feature_names=df.columns.tolist()
plt.figure(figsize=(40,20))
class_names=["class_0","class_1"]
plot_tree(dc, feature_names=feature_names, class_names=class_names, filled=True,fontsize=12)
plt.show()
## Decision Tree classifier

### using 'entropy' criterion
dc1=DecisionTreeClassifier(criterion='entropy',max_depth=5,min_samples_split=10)
dc1.fit(x_train,y_train)

## Evaluating Training and Testing Accuracy
print("Training accuracy:",dc1.score(x_train,y_train))
print("Testing accuracy:",dc1.score(x_test,y_test))
y1_pred=dc1.predict(x_test)
## Evaluating Prediction Accuracy
print(accuracy_score(y_test,y1_pred))
print(confusion_matrix(y_test,y1_pred))
print(classification_report(y_test,y1_pred))
cn=['no','yes']
fn=x_train.columns
plt.figure(figsize=(40,20))
plot_tree(dc1, feature_names=fn.tolist(), class_names=cn, filled=True,fontsize=12)
plt.show()
## `Final Conclusion`

- **High Accuracy for Both Criteria:** Both the Gini impurity and entropy criteria provide high accuracy for training and testing, with training accuracies around 93.6% and testing accuracies around 93.3% for Gini and 93.2% for entropy.
  


- **Performance Metrics Comparison:**
  - The Gini criterion has a slightly higher testing accuracy and a better recall for the positive class (1), indicating it is better at identifying true positive instances.
  - The entropy criterion results in fewer false positives but more false negatives compared to Gini.
